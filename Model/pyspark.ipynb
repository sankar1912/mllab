{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfecddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName('RegressionExample').getOrCreate()\n",
    "\n",
    "# Load data (example CSV file)\n",
    "data = spark.read.csv('your_data.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Select features and target variable\n",
    "features = ['feature1', 'feature2', 'feature3']  # Replace with your feature names\n",
    "target = 'target_column'  # Replace with your target column name\n",
    "\n",
    "# Assemble features into a vector column\n",
    "assembler = VectorAssembler(inputCols=features, outputCol='features')\n",
    "data = assembler.transform(data)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Initialize and train the linear regression model\n",
    "lr = LinearRegression(featuresCol='features', labelCol=target)\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Initialize RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=target, metricName=\"rmse\")\n",
    "\n",
    "# Calculate RMSE (Root Mean Squared Error)\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "\n",
    "# You can also calculate other metrics such as MSE, R2, and MAE:\n",
    "mse_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=target, metricName=\"mse\")\n",
    "mse = mse_evaluator.evaluate(predictions)\n",
    "print(f'Mean Squared Error (MSE): {mse}')\n",
    "\n",
    "r2_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=target, metricName=\"r2\")\n",
    "r2 = r2_evaluator.evaluate(predictions)\n",
    "print(f'R-squared (R2): {r2}')\n",
    "\n",
    "mae_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=target, metricName=\"mae\")\n",
    "mae = mae_evaluator.evaluate(predictions)\n",
    "print(f'Mean Absolute Error (MAE): {mae}')\n",
    "\n",
    "# Show predictions\n",
    "predictions.select('features', target, 'prediction').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba80da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName('ClusteringExample').getOrCreate()\n",
    "\n",
    "# Load data (example CSV file)\n",
    "data = spark.read.csv('your_data.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Select features for clustering\n",
    "features = ['feature1', 'feature2', 'feature3']  # Replace with your feature names\n",
    "\n",
    "# Assemble features into a vector column\n",
    "assembler = VectorAssembler(inputCols=features, outputCol='features')\n",
    "data = assembler.transform(data)\n",
    "\n",
    "# Initialize KMeans and set parameters\n",
    "kmeans = KMeans(k=3, featuresCol='features', predictionCol='prediction')  # Set k as the number of clusters\n",
    "\n",
    "# Fit the model\n",
    "kmeans_model = kmeans.fit(data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = kmeans_model.transform(data)\n",
    "\n",
    "# Initialize ClusteringEvaluator\n",
    "evaluator = ClusteringEvaluator(predictionCol='prediction', featuresCol='features', metricName='silhouette')\n",
    "\n",
    "# Calculate Silhouette Score\n",
    "silhouette_score = evaluator.evaluate(predictions)\n",
    "print(f'Silhouette Score: {silhouette_score}')\n",
    "\n",
    "# You can also calculate other clustering metrics such as Davies-Bouldin Index (DBI) if needed.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
